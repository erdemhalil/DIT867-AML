{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0e0c4f7f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-17T21:26:27.326152Z",
     "start_time": "2024-02-17T21:26:23.930309Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.models import Word2Vec\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "import contractions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a7f0a012",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-17T21:26:33.990563Z",
     "start_time": "2024-02-17T21:26:31.625794Z"
    }
   },
   "outputs": [],
   "source": [
    "# 训练Word2Vec模型\n",
    "w2v_model = Word2Vec(sentences=data['cleaned_text'], vector_size=300, window=5, min_count=1, workers=4)\n",
    "\n",
    "# 定义数据集类 w2v_model\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, labels, word2vec):\n",
    "        self.labels = labels.reset_index(drop=True)\n",
    "        self.texts = texts.reset_index(drop=True)\n",
    "        self.word2vec = word2vec\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "        vectors = [self.word2vec.wv[word] for word in text if word in self.word2vec.wv]\n",
    "        vectors_np = np.array(vectors, dtype=np.float32)  # 将列表转换为numpy数组\n",
    "        vectors_tensor = torch.from_numpy(vectors_np)  # 从numpy数组创建tensor\n",
    "        if len(vectors_tensor) > 200:\n",
    "            vectors_tensor = vectors_tensor[:200]\n",
    "        else:\n",
    "            padding_size = 200 - len(vectors_tensor)\n",
    "            padding = torch.zeros(padding_size, 300)  # 修改这里的维度为300，以匹配Word2Vec的输出\n",
    "            vectors_tensor = torch.cat((vectors_tensor, padding), dim=0)\n",
    "        return vectors_tensor, torch.tensor(label, dtype=torch.float)  # 注意: 对于BCEWithLogitsLoss，标签也应为float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "043143c4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-15T13:57:33.927650Z",
     "start_time": "2024-02-15T13:57:33.925317Z"
    }
   },
   "outputs": [],
   "source": [
    "# def load_glove_vectors(glove_file):\n",
    "#     \"\"\"加载GloVe词向量\"\"\"\n",
    "#     print(f\"Loading GloVe vectors from file: {glove_file}\")\n",
    "#     word2vec = {}\n",
    "#     with open(glove_file, 'r', encoding='utf-8') as f:\n",
    "#         for line in f:\n",
    "#             parts = line.split()\n",
    "#             word = parts[0]\n",
    "#             vector = np.array(parts[1:], dtype=np.float32)\n",
    "#             word2vec[word] = vector\n",
    "#     return word2vec\n",
    "\n",
    "# # 假设您的GloVe向量文件路径如下\n",
    "# glove_path = r'/Users/wilsonlee/Library/Mobile Documents/com~apple~CloudDocs/01哥德堡大学/Course by Periods/02_2023_Fall_P2/LT2114 - Practical Natural Language Processing/Lecture Notes/glove.twitter.27B/glove.twitter.27B.200d.txt'\n",
    "# glove_vectors = load_glove_vectors(glove_path)\n",
    "\n",
    "\n",
    "# # 定义数据集类 GloVe\n",
    "# class TextDataset(Dataset):\n",
    "#     def __init__(self, texts, labels, word2vec):\n",
    "#         self.labels = labels.reset_index(drop=True)\n",
    "#         self.texts = texts.reset_index(drop=True)\n",
    "#         self.word2vec = word2vec\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.texts)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         text = self.texts[idx]  # 这里text应该是一个包含单词的列表\n",
    "#         label = self.labels[idx]\n",
    "#         vectors = [self.word2vec[word] for word in text if word in self.word2vec]\n",
    "#         vectors_tensor = torch.tensor(vectors, dtype=torch.float)\n",
    "#         if len(vectors_tensor) > 200:\n",
    "#             vectors_tensor = vectors_tensor[:200]\n",
    "#         else:\n",
    "#             padding_size = 200 - len(vectors_tensor)\n",
    "#             padding = torch.zeros(padding_size, 200)  # 确保这里的维度匹配GloVe向量的大小\n",
    "#             vectors_tensor = torch.cat((vectors_tensor, padding), dim=0)\n",
    "#         return vectors_tensor, torch.tensor(label, dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d6725026",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-17T22:02:36.796795Z",
     "start_time": "2024-02-17T21:27:29.228203Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training RNN model...\n",
      "Epoch 1, Loss: 0.6989728875230976\n",
      "Epoch 2, Loss: 0.6961120840251102\n",
      "Epoch 3, Loss: 0.6971137715781511\n",
      "Epoch 4, Loss: 0.6980976008327167\n",
      "Epoch 5, Loss: 0.696090811014372\n",
      "Epoch 6, Loss: 0.6965836261484314\n",
      "Epoch 7, Loss: 0.6972623024710131\n",
      "Epoch 8, Loss: 0.6956991138088733\n",
      "Epoch 9, Loss: 0.6955934151184038\n",
      "Epoch 10, Loss: 0.6968284995715085\n",
      "Evaluating RNN model...\n",
      "Validation Accuracy: 0.5063904349618635\n",
      "Training LSTM model...\n",
      "Epoch 1, Loss: 0.6930777265921371\n",
      "Epoch 2, Loss: 0.6929294142660186\n",
      "Epoch 3, Loss: 0.692973527304908\n",
      "Epoch 4, Loss: 0.6928684537015665\n",
      "Epoch 5, Loss: 0.6928841976812823\n",
      "Epoch 6, Loss: 0.6928773963146728\n",
      "Epoch 7, Loss: 0.6928525570985805\n",
      "Epoch 8, Loss: 0.6928390090070474\n",
      "Epoch 9, Loss: 0.6928000762653272\n",
      "Epoch 10, Loss: 0.6927532335990535\n",
      "Evaluating LSTM model...\n",
      "Validation Accuracy: 0.5063904349618635\n",
      "Training BiLSTM model...\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "lstm() received an invalid combination of arguments - got (Tensor, tuple, list, bool, bool, float, bool, float, bool), but expected one of:\n * (Tensor data, Tensor batch_sizes, tuple of Tensors hx, tuple of Tensors params, bool has_biases, int num_layers, float dropout, bool train, bool bidirectional)\n      didn't match because some of the arguments have invalid types: (Tensor, !tuple of (Tensor, Tensor)!, !list of [Parameter, Parameter, Parameter, Parameter, Parameter, Parameter, Parameter, Parameter]!, !bool!, bool, !float!, !bool!, !float!, bool)\n * (Tensor input, tuple of Tensors hx, tuple of Tensors params, bool has_biases, int num_layers, float dropout, bool train, bool bidirectional, bool batch_first)\n      didn't match because some of the arguments have invalid types: (Tensor, !tuple of (Tensor, Tensor)!, !list of [Parameter, Parameter, Parameter, Parameter, Parameter, Parameter, Parameter, Parameter]!, bool, !bool!, float, bool, !float!, bool)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 163\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;66;03m# 训练BiLSTM模型\u001b[39;00m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining BiLSTM model...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 163\u001b[0m bilstm_losses \u001b[38;5;241m=\u001b[39m train_model(bilstm_model, bilstm_optimizer, loss_function, train_loader, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)\n\u001b[1;32m    165\u001b[0m \u001b[38;5;66;03m# 评估BiLSTM模型\u001b[39;00m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEvaluating BiLSTM model...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[6], line 78\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, optimizer, loss_function, train_loader, epochs)\u001b[0m\n\u001b[1;32m     76\u001b[0m vectors, labels \u001b[38;5;241m=\u001b[39m vectors\u001b[38;5;241m.\u001b[39mto(device), labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     77\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 78\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(vectors)\n\u001b[1;32m     79\u001b[0m outputs \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39msqueeze()  \u001b[38;5;66;03m# 使用squeeze()方法调整模型输出的尺寸\u001b[39;00m\n\u001b[1;32m     80\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_function(outputs, labels)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[6], line 54\u001b[0m, in \u001b[0;36mBiLSTMModel.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     51\u001b[0m h0, c0 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minit_hidden(x\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m))\n\u001b[1;32m     53\u001b[0m \u001b[38;5;66;03m# 前向传播\u001b[39;00m\n\u001b[0;32m---> 54\u001b[0m out, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlstm(x, (h0, c0))\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# 应用Dropout\u001b[39;00m\n\u001b[1;32m     57\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(out[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :])  \u001b[38;5;66;03m# 取最后一个时间步\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:878\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    875\u001b[0m         hx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpermute_hidden(hx, sorted_indices)\n\u001b[1;32m    877\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_sizes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 878\u001b[0m     result \u001b[38;5;241m=\u001b[39m _VF\u001b[38;5;241m.\u001b[39mlstm(\u001b[38;5;28minput\u001b[39m, hx, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flat_weights, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers,\n\u001b[1;32m    879\u001b[0m                       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbidirectional, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_first)\n\u001b[1;32m    880\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    881\u001b[0m     result \u001b[38;5;241m=\u001b[39m _VF\u001b[38;5;241m.\u001b[39mlstm(\u001b[38;5;28minput\u001b[39m, batch_sizes, hx, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flat_weights, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias,\n\u001b[1;32m    882\u001b[0m                       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbidirectional)\n",
      "\u001b[0;31mTypeError\u001b[0m: lstm() received an invalid combination of arguments - got (Tensor, tuple, list, bool, bool, float, bool, float, bool), but expected one of:\n * (Tensor data, Tensor batch_sizes, tuple of Tensors hx, tuple of Tensors params, bool has_biases, int num_layers, float dropout, bool train, bool bidirectional)\n      didn't match because some of the arguments have invalid types: (Tensor, !tuple of (Tensor, Tensor)!, !list of [Parameter, Parameter, Parameter, Parameter, Parameter, Parameter, Parameter, Parameter]!, !bool!, bool, !float!, !bool!, !float!, bool)\n * (Tensor input, tuple of Tensors hx, tuple of Tensors params, bool has_biases, int num_layers, float dropout, bool train, bool bidirectional, bool batch_first)\n      didn't match because some of the arguments have invalid types: (Tensor, !tuple of (Tensor, Tensor)!, !list of [Parameter, Parameter, Parameter, Parameter, Parameter, Parameter, Parameter, Parameter]!, bool, !bool!, float, bool, !float!, bool)\n"
     ]
    }
   ],
   "source": [
    "    \n",
    "\n",
    "# 定义RNN模型\n",
    "class RNNModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(RNNModel, self).__init__()\n",
    "        self.rnn = nn.RNN(input_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # 初始化隐藏状态\n",
    "        h0 = torch.zeros(1, x.size(0), hidden_dim)\n",
    "        # 前向传播\n",
    "        out, _ = self.rnn(x, h0)\n",
    "        out = self.fc(out[:, -1, :]) # 取最后一个时间步\n",
    "        return out\n",
    "\n",
    "# 定义LSTM模型\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # 初始化隐藏状态和细胞状态\n",
    "        h0 = torch.zeros(1, x.size(0), hidden_dim)\n",
    "        c0 = torch.zeros(1, x.size(0), hidden_dim)\n",
    "        # 前向传播\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        out = self.fc(out[:, -1, :]) # 取最后一个时间步\n",
    "        return out\n",
    "    \n",
    "class BiLSTMModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers=2, bidirectional=True, dropout_rate=0.5, device='cpu'):\n",
    "        super(BiLSTMModel, self).__init__()\n",
    "        self.device = torch.device(device)\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.bidirectional = bidirectional\n",
    "        \n",
    "        # 定义LSTM层\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers=num_layers, batch_first=True, bidirectional=bidirectional, dropout=dropout_rate if num_layers > 1 else 0).to(self.device)\n",
    "        \n",
    "        # 定义Dropout层\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        \n",
    "        # 定义全连接层，如果是双向，则维度是隐藏维度的两倍\n",
    "        self.fc = nn.Linear(hidden_dim * 2 if bidirectional else hidden_dim, output_dim).to(self.device)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # 初始化隐藏状态和细胞状态\n",
    "        h0, c0 = self.init_hidden(x.size(0))\n",
    "        \n",
    "        # 前向传播\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        \n",
    "        # 应用Dropout\n",
    "        out = self.dropout(out[:, -1, :])  # 取最后一个时间步\n",
    "        \n",
    "        # 通过全连接层\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        # 生成初始隐藏状态和细胞状态\n",
    "        num_directions = 2 if self.bidirectional else 1\n",
    "        h0 = torch.zeros(self.num_layers * num_directions, batch_size, self.hidden_dim).to(self.device)\n",
    "        c0 = torch.zeros(self.num_layers * num_directions, batch_size, self.hidden_dim).to(self.device)\n",
    "        return h0, c0\n",
    "\n",
    "def train_model(model, optimizer, loss_function, train_loader, epochs=10):\n",
    "    model.train()\n",
    "    train_losses = []\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for vectors, labels in train_loader:\n",
    "            vectors, labels = vectors.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(vectors)\n",
    "            outputs = outputs.squeeze()  # 使用squeeze()方法调整模型输出的尺寸\n",
    "            loss = loss_function(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        train_losses.append(avg_loss)\n",
    "        print(f'Epoch {epoch+1}, Loss: {avg_loss}')\n",
    "    return train_losses\n",
    "\n",
    "\n",
    "def evaluate_model(model, validation_loader, threshold=0.5):\n",
    "    model.eval()\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for vectors, labels in validation_loader:\n",
    "            vectors, labels = vectors.to(device), labels.to(device)\n",
    "            outputs = model(vectors).squeeze()  # 调整模型输出尺寸\n",
    "            predictions = torch.sigmoid(outputs) > threshold  # 将sigmoid激活后的输出转换为二进制预测\n",
    "            all_predictions.extend(predictions.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    accuracy = accuracy_score(all_labels, all_predictions)\n",
    "    print(f'Validation Accuracy: {accuracy}')\n",
    "    return accuracy\n",
    "\n",
    "# 准备数据集\n",
    "train_data, validation_data = train_test_split(data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Creating TextDataset instances for training and validation\n",
    "train_dataset = TextDataset(train_data['cleaned_text'], train_data['label'], w2v_model)\n",
    "validation_dataset = TextDataset(validation_data['cleaned_text'], validation_data['label'], w2v_model)\n",
    "# 使用GloVe词向量初始化训练和验证数据集\n",
    "# train_dataset = TextDataset(train_data['cleaned_text'], train_data['label'], glove_vectors)\n",
    "# validation_dataset = TextDataset(validation_data['cleaned_text'], validation_data['label'], glove_vectors)\n",
    "\n",
    "\n",
    "# Creating DataLoader instances for training and validation\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "validation_loader = DataLoader(validation_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Hyperparameters\n",
    "input_dim = 300  \n",
    "hidden_dim = 256\n",
    "output_dim = 1  # 对于二分类任务，确保这里为1\n",
    "bidirectional = True\n",
    "dropout_rate = 0.5\n",
    "\n",
    "# Check if GPU is available and move models to GPU if it is\n",
    "# device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 实例化模型\n",
    "rnn_model = RNNModel(input_dim, hidden_dim, output_dim).to(device)\n",
    "lstm_model = LSTMModel(input_dim, hidden_dim, output_dim).to(device)\n",
    "bilstm_model = BiLSTMModel(input_dim, hidden_dim, output_dim, bidirectional, dropout_rate).to(device)\n",
    "\n",
    "# 定义损失函数和优化器\n",
    "loss_function = nn.BCEWithLogitsLoss()\n",
    "\n",
    "rnn_optimizer = optim.Adam(rnn_model.parameters(), lr=0.001)\n",
    "lstm_optimizer = optim.Adam(lstm_model.parameters(), lr=0.001)\n",
    "bilstm_optimizer = optim.Adam(bilstm_model.parameters(), lr=0.001)\n",
    "\n",
    "\n",
    "# 训练和评估模型\n",
    "# 训练RNN模型\n",
    "print(\"Training RNN model...\")\n",
    "rnn_losses = train_model(rnn_model, rnn_optimizer, loss_function, train_loader, epochs=10)\n",
    "\n",
    "# 评估RNN模型\n",
    "print(\"Evaluating RNN model...\")\n",
    "rnn_accuracy = evaluate_model(rnn_model, validation_loader)\n",
    "\n",
    "# 训练LSTM模型\n",
    "print(\"Training LSTM model...\")\n",
    "lstm_losses = train_model(lstm_model, lstm_optimizer, loss_function, train_loader, epochs=10)\n",
    "\n",
    "# 评估LSTM模型\n",
    "print(\"Evaluating LSTM model...\")\n",
    "lstm_accuracy = evaluate_model(lstm_model, validation_loader)\n",
    "\n",
    "# 训练BiLSTM模型\n",
    "print(\"Training BiLSTM model...\")\n",
    "bilstm_losses = train_model(bilstm_model, bilstm_optimizer, loss_function, train_loader, epochs=10)\n",
    "\n",
    "# 评估BiLSTM模型\n",
    "print(\"Evaluating BiLSTM model...\")\n",
    "bilstm_accuracy = evaluate_model(bilstm_model, validation_loader)\n",
    "\n",
    "# 绘制训练损失\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(rnn_losses, label='RNN Loss')\n",
    "plt.plot(lstm_losses, label='LSTM Loss')\n",
    "plt.plot(bilstm_losses, label='BiLSTM Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Losses')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# 打印准确率\n",
    "print(f'RNN Validation Accuracy: {rnn_accuracy}')\n",
    "print(f'LSTM Validation Accuracy: {lstm_accuracy}')\n",
    "print(f'BiLSTM Validation Accuracy: {bilstm_accuracy}')"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
