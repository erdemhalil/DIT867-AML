{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b2202c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-18T14:03:42.398029Z",
     "start_time": "2024-02-18T14:03:34.951471Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sentiment_analyzer import analyze_sentiment_textblob, analyze_sentiment_vader,analyze_sentiment_flair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b22b69c4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-18T14:03:42.467664Z",
     "start_time": "2024-02-18T14:03:42.398947Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>annotations</th>\n",
       "      <th>comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1/1</td>\n",
       "      <td>I'll only consume if I know what's inside it....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0/-1</td>\n",
       "      <td>It is easier to fool a million people than it...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0/0</td>\n",
       "      <td>NATURAL IMMUNITY  protected us since evolutio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0/-1</td>\n",
       "      <td>NATURAL IMMUNITY  protected us since evolutio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0/0</td>\n",
       "      <td>Proud to have resisted. Proud of my husband, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  annotations                                            comment\n",
       "0         1/1   I'll only consume if I know what's inside it....\n",
       "1        0/-1   It is easier to fool a million people than it...\n",
       "2         0/0   NATURAL IMMUNITY  protected us since evolutio...\n",
       "3        0/-1   NATURAL IMMUNITY  protected us since evolutio...\n",
       "4         0/0   Proud to have resisted. Proud of my husband, ..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('a3_train_final.tsv', sep='\\t', header=None)\n",
    "data.columns = ['annotations', 'comment']\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cdea8f89",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-18T14:03:42.629456Z",
     "start_time": "2024-02-18T14:03:42.469039Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Krippendorff's alpha: 0.015974378717868\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import krippendorff\n",
    "\n",
    "# Parse original data into a 2D list and convert to floats\n",
    "data_list = [[float(n) for n in item.split('/')] for item in data.annotations.values]\n",
    "\n",
    "# Find the maximum number of annotators\n",
    "max_len = max(len(item) for item in data_list)\n",
    "\n",
    "# Fill lists shorter than the maximum length with np.nan to match the maximum length\n",
    "data_filled = [item + [np.nan]*(max_len-len(item)) for item in data_list]\n",
    "\n",
    "# Convert the data into a numpy array\n",
    "data_np = np.array(data_filled, dtype=float)\n",
    "\n",
    "# Calculate Krippendorff's alpha\n",
    "alpha = krippendorff.alpha(data_np, level_of_measurement='interval')\n",
    "\n",
    "print(\"Krippendorff's alpha:\", alpha)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6f48134",
   "metadata": {},
   "source": [
    "**Question:** How much consensus is there between annotators of the dataset? Do you think the data is reliable?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "083b4722",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-18T14:03:42.633489Z",
     "start_time": "2024-02-18T14:03:42.630958Z"
    }
   },
   "outputs": [],
   "source": [
    "def process_annotations(s):\n",
    "    '''\n",
    "    Principles of Preprocessing Annotations:\n",
    "        1. Single Annotator Reliability: Comments annotated by only a single individual are \n",
    "            considered unreliable. In such cases, a value of -1 is returned, signifying unreliability.\n",
    "\n",
    "        2. Consensus on Annotation: When a comment is annotated by multiple individuals and \n",
    "            all annotations agree (i.e., the annotation numbers are identical), the comment is \n",
    "            deemed reliable. The consensus number, which may be either 1 or 0, is returned to \n",
    "            indicate the comment's true sentiment.\n",
    "\n",
    "        3. Handling Disagreements:\n",
    "            3.1 Majority Rule: For comments annotated by multiple individuals where annotations \n",
    "                do not all agree, if over 50% of the annotations share the same label, that \n",
    "                majority label is considered reliable and returned.\n",
    "            3.2 Lack of Majority: If no single label accounts for more than 50% of the annotations, \n",
    "                the comment is deemed unreliable, and -1 is returned.\n",
    "        \n",
    "        4. Default Unreliability: In any situation not covered by the above rules, the comment \n",
    "            is considered unreliable and assigned a value of -1.\n",
    "\n",
    "        5. Management of Unreliable Comments: Comments deemed unreliable should either be \n",
    "            re-annotated or discarded, as their ambiguity or lack of consensus undermines \n",
    "            the integrity of the dataset.\n",
    "    '''\n",
    "    # if the comment is annotated by only one person, it is not reliable\n",
    "    if len(s) == 1:\n",
    "        return -1\n",
    "    \n",
    "    # Split the string by slashes and convert to integers\n",
    "    numbers = list(map(int, s.split('/')))\n",
    "    \n",
    "    # If all numbers are the same, return that number\n",
    "    if all(n == numbers[0] for n in numbers):\n",
    "        return numbers[0]\n",
    "    \n",
    "    # Calculate the frequency of each number\n",
    "    freq = {n: numbers.count(n) for n in set(numbers)}\n",
    "    \n",
    "    # If there's a number with frequency more than 50%, return it\n",
    "    most_common_number = max(freq.items(), key=lambda x: x[1])[0]\n",
    "    if freq[most_common_number] > len(numbers) / 2:\n",
    "        return most_common_number\n",
    "    \n",
    "    # Otherwise, return -1\n",
    "    return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "824fa25b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-18T14:03:42.670600Z",
     "start_time": "2024-02-18T14:03:42.634087Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 20878, 0: 20038, -1: 9152}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute the sentiment composition\n",
    "data[\"sentiment\"] = data.iloc[:,0].apply(lambda s: process_annotations(s))\n",
    "sentiment_composition = data.sentiment.value_counts().to_dict()\n",
    "sentiment_composition\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c86a1970",
   "metadata": {},
   "source": [
    "To quantify the level of agreement among annotators, we calculated the 'consensus proportion' within our dataset. This metric reflects the percentage of comments for which annotators reached a unanimous or majority agreement on the sentiment classification. A high 'consensus proportion' indicates a strong agreement among the annotators, suggesting that the dataset is reliable for further analysis. Conversely, a low 'consensus proportion' may indicate discrepancies in annotation, highlighting areas where the data might require reannotation or more careful review to ensure reliability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7ee10814",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-18T14:03:42.673584Z",
     "start_time": "2024-02-18T14:03:42.671709Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The consensus proportion of this annotated data set is 0.8172085963090198.\n"
     ]
    }
   ],
   "source": [
    "consensus_proportion = 1 - sentiment_composition[-1] / sum(sentiment_composition.values())\n",
    "print(f'The consensus proportion of this annotated data set is {consensus_proportion}.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a43d03",
   "metadata": {},
   "source": [
    "The consensus proportion of this annotated data set is 81.72%, which indicates that this data is reliable to a certain extent, although this ratio is not dramatically high.\n",
    "\n",
    "\n",
    "In adherence to the principle of retaining as much data as possible for training rather than simply discarding it, we will employ `TextBlob`, `vaderSentiment` and `Flair` to simulate three anonymous annotators further annotating those data deemed unreliable. Subsequently, we will reapply the Principles of Preprocessing Annotations. Should a portion of the results emerge as reliable, we intend to incorporate this subset into our training dataset. Conversely, if some data still render as unreliable upon reevaluation, we will proceed to discard this subset. This strategy aims to maximize the utility and size of our training dataset, enhancing the robustness and accuracy of our analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a684e5f6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-18T14:05:20.092146Z",
     "start_time": "2024-02-18T14:03:42.681259Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The new sentiment composition is {0: 24744, 1: 23766, -1: 1558}\n",
      "The consensus proportion of this annotated data set is 0.9688823200447392.\n"
     ]
    }
   ],
   "source": [
    "# Filter the DataFrame to only include rows where the sentiment is equal to -1 and create a copy to avoid SettingWithCopyWarning\n",
    "filtered_df = data[data[\"sentiment\"] == -1].copy()\n",
    "\n",
    "# Apply the sentiment analysis function to the 'comment' column of the filtered DataFrame\n",
    "filtered_df['sentiment_1'] = filtered_df['comment'].apply(analyze_sentiment_textblob)\n",
    "filtered_df['sentiment_2'] = filtered_df['comment'].apply(analyze_sentiment_vader)\n",
    "filtered_df['sentiment_3'] = analyze_sentiment_flair(filtered_df['comment'].tolist())\n",
    "\n",
    "# Concatenate the sentiment result with the 'annotations' column\n",
    "filtered_df['annotations'] = filtered_df.apply(lambda row: f\"{row['annotations']}/{row['sentiment_1']}/{row['sentiment_2']}/{row['sentiment_3']}\", axis=1)\n",
    "\n",
    "# Compute the final sentiment label\n",
    "filtered_df[\"sentiment\"] = filtered_df['annotations'].apply(process_annotations)\n",
    "\n",
    "# Update the data DataFrame, replace the data.sentiment where its value is equal to -1 with a second round annotated label\n",
    "data.loc[filtered_df.index, \"sentiment\"] = filtered_df[\"sentiment\"]\n",
    "data.loc[filtered_df.index, \"annotations\"] = filtered_df[\"annotations\"]\n",
    "\n",
    "# Recompute the sentiment composition and consensus proportion\n",
    "sentiment_composition = data.sentiment.value_counts().to_dict()\n",
    "print(f'The new sentiment composition is {sentiment_composition}')\n",
    "consensus_proportion = 1 - sentiment_composition[-1] / sum(sentiment_composition.values())\n",
    "print(f'The consensus proportion of this annotated data set is {consensus_proportion}.')\n",
    "\n",
    "# Disgard the unreliable data where the sentiment label is equal to -1\n",
    "new_data = data[data.sentiment != -1].iloc[:,1:]\n",
    "new_data = new_data[['sentiment', 'comment']]\n",
    "\n",
    "# Save the new data to a .csv file\n",
    "# new_data.to_csv('a3_train.csv', index=False, header=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a1d09e1d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-18T14:05:20.099014Z",
     "start_time": "2024-02-18T14:05:20.093089Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2039, 2)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = pd.read_csv('a3_test.tsv', sep='\\t', header=None)\n",
    "test_data.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "36521f2c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-18T14:05:20.284754Z",
     "start_time": "2024-02-18T14:05:20.107607Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Krippendorff's alpha: 0.015065998008823689\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import krippendorff\n",
    "\n",
    "# Parse original data into a 2D list and convert to floats\n",
    "data_list = [[float(n) for n in item.split('/')] for item in data.annotations.values]\n",
    "\n",
    "# Find the maximum number of annotators\n",
    "max_len = max(len(item) for item in data_list)\n",
    "\n",
    "# Fill lists shorter than the maximum length with np.nan to match the maximum length\n",
    "data_filled = [item + [np.nan]*(max_len-len(item)) for item in data_list]\n",
    "\n",
    "# Convert the data into a numpy array\n",
    "data_np = np.array(data_filled, dtype=float)\n",
    "\n",
    "# Calculate Krippendorff's alpha\n",
    "alpha = krippendorff.alpha(data_np, level_of_measurement='interval')\n",
    "\n",
    "print(\"Krippendorff's alpha:\", alpha)"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
